{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11005046,"sourceType":"datasetVersion","datasetId":6850997},{"sourceId":11160616,"sourceType":"datasetVersion","datasetId":6963969}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Subset, DataLoader\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:49:54.546280Z","iopub.execute_input":"2025-03-26T05:49:54.546602Z","iopub.status.idle":"2025-03-26T05:50:03.612628Z","shell.execute_reply.started":"2025-03-26T05:49:54.546575Z","shell.execute_reply":"2025-03-26T05:50:03.611956Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset = datasets.ImageFolder(root='/kaggle/input/homedataset')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:03.613794Z","iopub.execute_input":"2025-03-26T05:50:03.614180Z","iopub.status.idle":"2025-03-26T05:50:03.676721Z","shell.execute_reply.started":"2025-03-26T05:50:03.614148Z","shell.execute_reply":"2025-03-26T05:50:03.675899Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:03.678567Z","iopub.execute_input":"2025-03-26T05:50:03.678773Z","iopub.status.idle":"2025-03-26T05:50:03.684045Z","shell.execute_reply.started":"2025-03-26T05:50:03.678755Z","shell.execute_reply":"2025-03-26T05:50:03.683365Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset ImageFolder\n    Number of datapoints: 31\n    Root location: /kaggle/input/homedataset"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset\nimport os\n\n# Load the dataset\nds = load_dataset(\"ellljoy/interior-design\")\n\n# Define a directory to save images\nsave_dir = \"downloaded_images\"\nos.makedirs(save_dir, exist_ok=True)\n\n# Iterate over the dataset and save images\nfor i, image in enumerate(ds[\"train\"][\"images\"]):  \n    image_path = os.path.join(save_dir, f\"image_{i}.png\")  # Ensure correct extension\n    image.save(image_path)  # Directly save the PIL image\n\nprint(f\"Downloaded {len(ds['train'])} images to {save_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:03.684931Z","iopub.execute_input":"2025-03-26T05:50:03.685105Z","iopub.status.idle":"2025-03-26T05:50:15.302068Z","shell.execute_reply.started":"2025-03-26T05:50:03.685089Z","shell.execute_reply":"2025-03-26T05:50:15.301057Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7be31273532465a970f6a78dfd8d759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-dda5b1ba796822a6.parquet:   0%|          | 0.00/45.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d4f58c267f4057afd6137132cf41e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"209ea6ab9ae54d31ae72035b4086c90e"}},"metadata":{}},{"name":"stdout","text":"Downloaded 30 images to downloaded_images\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:15.303014Z","iopub.execute_input":"2025-03-26T05:50:15.303521Z","iopub.status.idle":"2025-03-26T05:50:15.308991Z","shell.execute_reply.started":"2025-03-26T05:50:15.303495Z","shell.execute_reply":"2025-03-26T05:50:15.307868Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset ImageFolder\n    Number of datapoints: 31\n    Root location: /kaggle/input/homedataset"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPTokenizer\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ntokenizer = CLIPTokenizer.from_pretrained(\n        model_id,\n        subfolder=\"tokenizer\")\n\ndef collate_fn(examples):\n    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n    pixel_values = [example[\"instance_images\"] for example in examples]\n    pixel_values = torch.stack(pixel_values)\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n    input_ids = tokenizer.pad(\n        {\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\"\n    ).input_ids\n\n    batch = {\n        \"input_ids\": input_ids,\n        \"pixel_values\": pixel_values,\n    }\n    return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:15.310098Z","iopub.execute_input":"2025-03-26T05:50:15.310393Z","iopub.status.idle":"2025-03-26T05:50:20.642251Z","shell.execute_reply.started":"2025-03-26T05:50:15.310367Z","shell.execute_reply":"2025-03-26T05:50:20.641516Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a488cc0612744ef1b7481565bc6b867c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69150c34745b47d7a0439bdaf09d2f76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af5d004d508d4467aaaa06411efc06bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d84e0e524814e7cb0eef325b1209c7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784561b5a52445849607d30034c4e9a2"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\n# def push_data_to_hf_hub(dataset_name,local_data_dir):\n#     dataset = load_dataset(\"imagefolder\", data_dir=local_data_dir)\n#     # Remove the dummy label column\n#     dataset = dataset.remove_columns(\"label\")\n#     # Push to Hub\n#     dataset.push_to_hub(dataset_name)\n\n# def pull_dataset_from_hf_hub(dataset_id='StatsGary/dreambooth-hackathon-images'):\n#     dataset_id = dataset_id\n#     dataset = load_dataset(dataset_id, split=\"train\")\n#     print(f\"Loaded dataset number of rows: {len(dataset)}\")\n#     return dataset\n\n\nclass DreamBoothDataset(Dataset):\n    def __init__(self, dataset, instance_prompt, tokenizer, size=512):\n        self.dataset = dataset\n        self.instance_prompt = instance_prompt\n        self.tokenizer = tokenizer\n        self.size = size\n        self.transforms = transforms.Compose(\n            [\n                transforms.Resize(size),\n                transforms.CenterCrop(size),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5], [0.5]),\n            ]\n        )\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        example = {}\n        image, _ = self.dataset[index]  # Unpack the tuple (ignore the label)\n        example[\"instance_images\"] = self.transforms(image)\n        example[\"instance_prompt_ids\"] = self.tokenizer(self.instance_prompt, return_tensors=\"pt\").input_ids\n        return example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:20.642993Z","iopub.execute_input":"2025-03-26T05:50:20.643340Z","iopub.status.idle":"2025-03-26T05:50:20.650679Z","shell.execute_reply.started":"2025-03-26T05:50:20.643321Z","shell.execute_reply":"2025-03-26T05:50:20.649842Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from PIL import Image\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows * cols\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n    grid_w, grid_h = grid.size\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:20.651587Z","iopub.execute_input":"2025-03-26T05:50:20.651895Z","iopub.status.idle":"2025-03-26T05:50:20.673594Z","shell.execute_reply.started":"2025-03-26T05:50:20.651865Z","shell.execute_reply":"2025-03-26T05:50:20.672943Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"pip install bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:20.675801Z","iopub.execute_input":"2025-03-26T05:50:20.675998Z","iopub.status.idle":"2025-03-26T05:50:29.327909Z","shell.execute_reply.started":"2025-03-26T05:50:20.675981Z","shell.execute_reply":"2025-03-26T05:50:29.326983Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#Define training loop\nimport math\nimport torch.nn.functional as F\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom diffusers import DDPMScheduler, PNDMScheduler, StableDiffusionPipeline\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport bitsandbytes as bnb\nimport torch\n\n\ndef train_dreambooth(text_encoder, vae, unet, tokenizer, feature_extractor, train_dataset, train_batch_size=1, max_train_steps=400, shuffle_train=True,\n                        beta_start=0.00085, beta_end=0.012, beta_scheduler=\"scaled_linear\", num_train_timesteps=1000, seed=3434554,\n                        gradient_checkpoint=True, gradient_accumulation_steps=8, use_8bit_ADAM=True, \n                        learning_rate=2e-06, max_grad_norm=1.0, output_dir='stable-diffusion-trained'):\n\n    # Takes the input from the training arguments to specify the warmup phase of the gradients\n    accelerator = Accelerator(\n        gradient_accumulation_steps=gradient_accumulation_steps,\n    )\n    # Sets a reproduable seed to work \n    set_seed(seed)\n    if gradient_checkpoint:\n        unet.enable_gradient_checkpointing()\n\n    if use_8bit_ADAM:\n        optimizer_class = bnb.optim.AdamW8bit\n    else:\n        optimizer_class = torch.optim.AdamW\n\n    # Then we implemenet and optimizer class which is used with the learning rate\n    optimizer = optimizer_class(\n        unet.parameters(),  # only optimize unet\n        lr=learning_rate,\n    )\n    # Create a random noise scheduler to be applied to the images\n    noise_scheduler = DDPMScheduler(\n        beta_start=beta_start,\n        beta_end=beta_end,\n        beta_schedule=beta_scheduler,\n        num_train_timesteps=num_train_timesteps\n    )\n\n    # Pass the images into the training data loader\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=train_batch_size,\n        shuffle=shuffle_train,\n        collate_fn=collate_fn\n    )\n\n    unet, optimizer, train_dataloader = accelerator.prepare(\n        unet, optimizer, train_dataloader\n    )\n\n    # Move text_encode and vae to gpu\n    text_encoder.to(accelerator.device)\n    vae.to(accelerator.device)\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(\n        len(train_dataloader) / gradient_accumulation_steps\n    )\n    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n\n    # Train!\n    total_batch_size = (\n        train_batch_size\n        * accelerator.num_processes\n        * gradient_accumulation_steps\n    )\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(\n        range(max_train_steps), disable=not accelerator.is_local_main_process\n    )\n    progress_bar.set_description(f\"Steps based on batch size {total_batch_size}\")\n    global_step = 0\n\n    # Set the training loop for each epoch\n    for epoch in range(num_train_epochs):\n        print(f'Epoch: {epoch + 1} of {num_train_epochs}')\n        unet.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(unet):\n                # Convert images to latent space\n                with torch.no_grad():\n                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n                    latents = latents * 0.18215\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn(latents.shape).to(latents.device)\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(\n                    0,\n                    noise_scheduler.config.num_train_timesteps,\n                    (bsz,),\n                    device=latents.device,\n                ).long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Get the text embedding for conditioning\n                with torch.no_grad():\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                # Predict the noise residual\n                noise_pred = unet(\n                    noisy_latents, timesteps, encoder_hidden_states\n                ).sample\n                loss = (\n                    F.mse_loss(noise_pred, noise, reduction=\"none\")\n                    .mean([1, 2, 3])\n                    .mean()\n                )\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n            logs = {\"loss\": loss.detach().item()}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= max_train_steps:\n                break\n\n        accelerator.wait_for_everyone()\n\n    # Create the pipeline using using the trained modules and save it.\n    if accelerator.is_main_process:\n        print(f\"Loading pipeline and saving to {output_dir}...\")\n        scheduler = PNDMScheduler(\n            beta_start=beta_start,\n            beta_end=beta_end,\n            beta_schedule=beta_scheduler,\n            skip_prk_steps=True,\n            steps_offset=1,\n        )\n        pipeline = StableDiffusionPipeline(\n            text_encoder=text_encoder,\n            vae=vae,\n            unet=accelerator.unwrap_model(unet),\n            tokenizer=tokenizer,\n            scheduler=scheduler,\n            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\n                \"CompVis/stable-diffusion-safety-checker\"\n            ),\n            feature_extractor=feature_extractor,\n        )\n        pipeline.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:29.329643Z","iopub.execute_input":"2025-03-26T05:50:29.329886Z","iopub.status.idle":"2025-03-26T05:50:51.280534Z","shell.execute_reply.started":"2025-03-26T05:50:29.329864Z","shell.execute_reply":"2025-03-26T05:50:51.279589Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# DreamBooth portion completed","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom transformers import CLIPFeatureExtractor, CLIPTextModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n  'stable_diffusion_backbone': 'CompVis/stable-diffusion-v1-4',\n  'feature_extractor': 'openai/clip-vit-base-patch32',\n  'hugging_face_image_store': 'StatsGary/dreambooth-hackathon-images',\n  'learning_rate' : 2e-06,\n  'max_train_steps' : 400,\n  'resolution' : 512,\n  'train_bs': 1,\n  'grad_accum_steps': 8,\n  'max_gradient_norm': 1.0,\n  'sample_batch_size': 10,\n  'model_checkpoint_name' : 'norweigen-fjords-dreambooth', # Not changed\n  'random_shuffle_train_set': True,\n  'use_8bit_optimizer': True ,\n  'concept_name': 'Interior Design',\n  'item_type': 'Rooms', #Change to person, cartoon, food, etc.\n  'eval_params':{\n  'image_save_path': 'images',\n  'eval_prompt': 'Dining table at the center classic paintings on each side of the wall of a room'\n  }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:51.288209Z","iopub.execute_input":"2025-03-26T05:50:51.288451Z","iopub.status.idle":"2025-03-26T05:50:51.318723Z","shell.execute_reply.started":"2025-03-26T05:50:51.288404Z","shell.execute_reply":"2025-03-26T05:50:51.317964Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_params = params\nSTABLE_DIFFUSION_NAME = train_params['stable_diffusion_backbone']\nFEATURE_EXTRACTOR = train_params['feature_extractor']\nhf_data_location = train_params['hugging_face_image_store']\nlearning_rate = float(train_params['learning_rate'])\nmax_train_steps = int(train_params['max_train_steps'])\nresolution = int(train_params['resolution'])\ntrain_batch_size=int(train_params['train_bs'])\ngrad_accum_steps=int(train_params['grad_accum_steps'])\nmax_gradient_norm=float(train_params['max_gradient_norm'])\nsample_batch_size=int(train_params['sample_batch_size'])\nmodel_checkpoint_name=str(train_params['model_checkpoint_name'])\nshuffle_train=bool(train_params['random_shuffle_train_set'])\nuse_8bit_optim=bool(train_params['use_8bit_optimizer'])\nname_of_your_concept=train_params['concept_name']\nobject_type=train_params['item_type']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:50:51.319496Z","iopub.execute_input":"2025-03-26T05:50:51.319802Z","iopub.status.idle":"2025-03-26T05:50:51.342787Z","shell.execute_reply.started":"2025-03-26T05:50:51.319774Z","shell.execute_reply":"2025-03-26T05:50:51.342082Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"if __name__ =='__main__':\n    # Load the image dataset from HuggingFace hub\n\n    # Name your concept and set of images\n    name_of_your_concept = name_of_your_concept\n    type_of_thing = object_type\n    instance_prompt = f\"a photo of {name_of_your_concept} {type_of_thing}\"\n    print(f\"Instance prompt: {instance_prompt}\")\n\n    # Load the CLIP tokenizer\n    model_id = STABLE_DIFFUSION_NAME\n    tokenizer = CLIPTokenizer.from_pretrained(\n        model_id,\n        subfolder=\"tokenizer\")\n\n    # Create a train dataset from the Dreambooth data loader\n    train_dataset = DreamBoothDataset(dataset, instance_prompt, tokenizer)\n\n    # Get text encoder, UNET and VAE\n    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n    feature_extractor = CLIPFeatureExtractor.from_pretrained(FEATURE_EXTRACTOR)\n\n    #Train the model\n    model = train_dreambooth(\n        text_encoder=text_encoder, \n        vae = vae, \n        unet = unet, \n        tokenizer=tokenizer, \n        feature_extractor=feature_extractor, \n        train_dataset=train_dataset, \n        train_batch_size=train_batch_size,\n        max_train_steps=max_train_steps, \n        shuffle_train=shuffle_train,\n        gradient_accumulation_steps=grad_accum_steps, \n        use_8bit_ADAM=True, \n        learning_rate=learning_rate, \n        max_grad_norm=max_gradient_norm,\n        output_dir=model_checkpoint_name\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T07:21:44.947480Z","iopub.execute_input":"2025-03-26T07:21:44.947770Z"}},"outputs":[{"name":"stdout","text":"Instance prompt: a photo of Interior Design Rooms\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/400 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ececb1bdbedf4a8e9cd6ff8045a4698b"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 of 100\nEpoch: 2 of 100\nEpoch: 3 of 100\nEpoch: 4 of 100\nEpoch: 5 of 100\nEpoch: 6 of 100\nEpoch: 7 of 100\nEpoch: 8 of 100\nEpoch: 9 of 100\nEpoch: 10 of 100\nEpoch: 11 of 100\nEpoch: 12 of 100\nEpoch: 13 of 100\nEpoch: 14 of 100\nEpoch: 15 of 100\nEpoch: 16 of 100\nEpoch: 17 of 100\nEpoch: 18 of 100\nEpoch: 19 of 100\nEpoch: 20 of 100\nEpoch: 21 of 100\nEpoch: 22 of 100\nEpoch: 23 of 100\nEpoch: 24 of 100\nEpoch: 25 of 100\nEpoch: 26 of 100\nEpoch: 27 of 100\nEpoch: 28 of 100\nEpoch: 29 of 100\nEpoch: 30 of 100\nEpoch: 31 of 100\nEpoch: 32 of 100\nEpoch: 33 of 100\nEpoch: 34 of 100\nEpoch: 35 of 100\nEpoch: 36 of 100\nEpoch: 37 of 100\nEpoch: 38 of 100\nEpoch: 39 of 100\nEpoch: 40 of 100\nEpoch: 41 of 100\nEpoch: 42 of 100\nEpoch: 43 of 100\nEpoch: 44 of 100\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from diffusers import StableDiffusionPipeline\nimport torch\nimport os\n\n# Parameters\neval_prompt = params['eval_params']['eval_prompt']\nimage_save_path = params['eval_params']['image_save_path']\nmodel_checkpoint_name = params['model_checkpoint_name']\nsample_batch_size = params['sample_batch_size']\n\n# Ensure the image save directory exists\nos.makedirs(image_save_path, exist_ok=True)\n\n# Load the trained pipeline\npipeline = StableDiffusionPipeline.from_pretrained(\n    model_checkpoint_name,\n    torch_dtype=torch.float16,  # Use FP16 for faster inference\n).to(\"cuda\")\n\n# Generate images\nfor i in range(sample_batch_size):\n    print(f\"Generating image {i + 1}...\")\n    image = pipeline(eval_prompt).images[0]\n    image.save(os.path.join(image_save_path, f\"generated_image_{i + 1}.png\"))\n\nprint(f\"Generated images saved to {image_save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}